{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 4: Natural Language Processing and Unsupervised Learning.\n",
    "\n",
    "In this notebook, I scrape Reddit r/medicine for the top 200 (by number of comments) posts made each month from January 2020 to July 2020. Then, I said this into a PostgreSQL database for easy data access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Imports](#section1)\n",
    "2. [Reddit Authorization](#section2)\n",
    "3. [Scraping Submissions](#section3)\n",
    "4. [Transferring to SQL Database](#section4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# working with UTC time for reddit posts\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "\n",
    "# scraping\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# package specifically made for scraping reddit\n",
    "import praw\n",
    "\n",
    "# importing into SQL Database\n",
    "import psycopg2 as pg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reddit Authorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get reddit credentials with my client_id, api_key, username, and password\n",
    "# you may need to use your own client_id, api_id, etc to get access to reddit's API\n",
    "with open('secret/reddit_credentials.json') as open_file:\n",
    "    params = json.load(open_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=params['client_id'],\n",
    "                     client_secret=params['api_key'],\n",
    "                     username=params['username'],\n",
    "                     password=params['password'],\n",
    "                     user_agent=\"macbook:jackynlpexploration (by u/TheLittleYoshi)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scraping Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I write several functions to scrape the r/medicine subreddit for each month of 2020 so far (January to July). My workflow is as follows:<br/>\n",
    "1. Grab the UTC time for the start and end of each month\n",
    "2. Use the month start and end times to access reddit pushshift API and get r/medicine post information corresponding to that month\n",
    "3. Turn the post information into an organized DataFrame\n",
    "4. Use reddit PRAW python package to fill in any submission text that pushshift API may be missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utc_month_times(month):\n",
    "    '''\n",
    "    Reddit submissions use UTC time. This function returns the UTC time corresponding to the first day\n",
    "    of a month and the first day of the next month. The goal is to use this time to capture all submissions\n",
    "    made within a certain month. For example, posts made in March would be between March 1st UTC time and \n",
    "    April 1st UTC time.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    - month (int): integer corresponding to the month of the year\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    - utc_month_start (int): UTC time for the start of the month\n",
    "    - utc_month_end (int): UTC time for the start of the next month\n",
    "    '''\n",
    "    month_start = datetime(2020, month, 1)\n",
    "    utc_month_start = int(month_start.replace(tzinfo=timezone.utc).timestamp())\n",
    "    \n",
    "    month_end = datetime(2020, month+1, 1)\n",
    "    utc_month_end = int(month_end.replace(tzinfo=timezone.utc).timestamp())\n",
    "    \n",
    "    return (utc_month_start, utc_month_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_submission_list(json_response, month):\n",
    "    '''\n",
    "    Use reddit pushshift API's JSON object to get post information and return a list of dictionaries\n",
    "    that can be used to turn into a pandas DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    - json_response (json object): json request from pushshift API\n",
    "    - month (int): integer corresponding to month of the year\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    - submission_list (list): list of dictionaries with post information\n",
    "    '''\n",
    "    submission_list = []\n",
    "    \n",
    "    for i in range(len(json_response['data'])):\n",
    "        submission_dict = {}\n",
    "        submission_dict['post_id'] = json_response['data'][i]['id']\n",
    "        submission_dict['author'] = json_response['data'][i]['author']\n",
    "        submission_dict['created_utc'] = json_response['data'][i]['created_utc'] \n",
    "        submission_dict['month'] = month\n",
    "        submission_dict['title'] = str(json_response['data'][i]['title'])\n",
    "        \n",
    "        if 'selftext' in json_response['data'][i]:\n",
    "            submission_dict['submission_text'] = str(json_response['data'][i]['selftext'])\n",
    "        else:\n",
    "            submission_dict['submission_text'] = ''\n",
    "            \n",
    "        submission_dict['score'] = json_response['data'][i]['score']\n",
    "        submission_dict['num_comments'] = json_response['data'][i]['num_comments']\n",
    "        submission_dict['stickied'] = json_response['data'][i]['stickied']\n",
    "        submission_dict['url'] = json_response['data'][i]['full_link']\n",
    "        \n",
    "        submission_list.append(submission_dict)\n",
    "    \n",
    "    return submission_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_monthly_submissions(subreddit, month, sort_type, sort):\n",
    "    '''\n",
    "    Scrape reddit pushshiftAPI for post information to get a Dataframe with submission information about\n",
    "    each post including month, title, text. The PushShiftAPI has some weird post limits. At the time of this\n",
    "    notebook, each API request can only return a maximum of 100 posts. To order to grab more posts, I make\n",
    "    2 requests: one for the first half of the month and one for the second half.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    - subreddit (string): name of subreddit\n",
    "    - month (int): integer corresponding to the month of the year\n",
    "    - sort_type (string): what to sort by (should be one of 'num_comments', 'score', or 'created_utc')\n",
    "    - sort (string): should be one of 'asc' (ascending) or 'desc' (descending)\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    - submission_df (DataFrame): Dataframe with subreddit post information\n",
    "    \n",
    "    '''\n",
    "\n",
    "    \n",
    "    url_template = (r'https://api.pushshift.io/reddit/search/submission/?size=100&subreddit={}&after={}&before={}'\n",
    "                    '&sort_type={}&sort={}')\n",
    "    \n",
    "    start_time = utc_month_times(month)[0]\n",
    "    end_time = utc_month_times(month)[1]\n",
    "    mid_time = int((end_time + start_time)/2)\n",
    "    \n",
    "    # request for top 100 posts made in the first half of the month\n",
    "    first_url = url_template.format(subreddit, start_time, mid_time, sort_type, sort)\n",
    "    first_request = requests.get(first_url)\n",
    "    assert first_request.status_code == 200\n",
    "    \n",
    "    first_json_response = first_request.json()\n",
    "    first_submission_list = fill_submission_list(first_json_response, month)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    # request for top 100 posts made in the second half of the month\n",
    "    second_url = url_template.format(subreddit, mid_time, end_time, sort_type, sort)\n",
    "    second_request = requests.get(second_url)\n",
    "    assert second_request.status_code == 200\n",
    "    \n",
    "    second_json_response = second_request.json()\n",
    "    second_submission_list = fill_submission_list(second_json_response, month)\n",
    "    \n",
    "    submission_list = first_submission_list + second_submission_list\n",
    "    submission_df = pd.DataFrame(submission_list)\n",
    "    \n",
    "    # single quotes break my sql query;\n",
    "    submission_df[\"title\"] = submission_df[\"title\"].apply(lambda x: str(x).replace(\"'\",''))\n",
    "    submission_df[\"submission_text\"] = submission_df[\"submission_text\"].apply(lambda x: str(x).replace(\"'\",''))\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_removed_text(submission_df):\n",
    "    '''\n",
    "    PushshiftAPI posts may have \"[removed]\" in the submission_text field. Fill this information\n",
    "    in using reddit Python package PRAW. \n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    - submission_df (DataFrame): Dataframe with subreddit post information (may have [removed] as submission text)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    - submission_df (DataFrame): Dataframe with subreddit post information (submission text filled in)\n",
    "    '''\n",
    "    \n",
    "    missing_text = submission_df.loc[submission_df[\"submission_text\"] == '[removed]', 'post_id'].values\n",
    "    \n",
    "    for post_id in missing_text:\n",
    "        praw_submission = reddit.submission(id=post_id)\n",
    "        filled_text = praw_submission.selftext\n",
    "        # single quotes break my sql query\n",
    "        submission_df.loc[submission_df['post_id'] == post_id, 'submission_text'] = filled_text.replace(\"'\",'')\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transferring to SQL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I use the functions I created above to scrape reddit and store the information in a SQL database that I create. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE DATABASE\r\n",
      "You are now connected to database \"reddit_medicine\" as user \"jackylu\".\r\n",
      "CREATE TABLE\r\n"
     ]
    }
   ],
   "source": [
    "# create database and tables\n",
    "!psql -f reddit_medicine.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postgres info to connect\n",
    "\n",
    "connection_args = {\n",
    "    'host': 'localhost', # We are connecting to our local version of psql\n",
    "    'dbname': 'reddit_medicine',        # DB that we are connecting to\n",
    "    'port': 5432,        # port we opened on AWS\n",
    "    'password':'',\n",
    "    'user': 'postgres'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_postgres(connection_args):\n",
    "    '''\n",
    "    Connect to PostgreSQL database server\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    - connection_args (dict): dictionary with information needed to connect to the postgreSQL database\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    - connection (psycopg2 connection object): the connection to the postgreSQL database\n",
    "    \n",
    "    '''\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = pg.connect(**connection_args)\n",
    "    except (Exception, pg.DatabaseError) as error:\n",
    "        print(error)\n",
    "        \n",
    "    return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_to_sql(submission_df, table, connection):\n",
    "    '''\n",
    "    Transfer the information from a pandas Dataframe to a PostgreSQL table.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    - submission_df (Dataframe): dataframe with reddit post data\n",
    "    - table (str): name of the PostgreSQL table to populate with data\n",
    "    - connection (psycopg2 object): connectioni to the PostgreSQL database\n",
    "    \n",
    "    '''\n",
    "    cursor = connection.cursor()\n",
    "    tuple_list = [tuple(x) for x in submission_df.to_numpy()]\n",
    "    \n",
    "    for tup in tuple_list:\n",
    "        sql_query = (f\" INSERT INTO {table} VALUES ('{tup[0]}','{tup[1]}','{tup[2]}','{tup[3]}','{tup[4]}',\\\n",
    "                    '{tup[5]}','{tup[6]}','{tup[7]}','{tup[8]}','{tup[9]}');\")\n",
    "        try:\n",
    "            cursor.execute(sql_query)\n",
    "            cursor.execute('commit;')\n",
    "        except (Exception, pg.DatabaseError) as error:\n",
    "            print(error)\n",
    "            print(tup[0])\n",
    "            connection.rollback()\n",
    "            cursor.close()\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = connect_to_postgres(connection_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_df = scrape_monthly_submissions('medicine', 1, 'num_comments', 'desc')\n",
    "jan_df = fill_in_removed_text(jan_df)\n",
    "database_to_sql(jan_df, 'submissions', connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_df = scrape_monthly_submissions('medicine', 2, 'num_comments', 'desc')\n",
    "feb_df = fill_in_removed_text(feb_df)\n",
    "database_to_sql(feb_df, 'submissions', connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "march_df = scrape_monthly_submissions('medicine', 3, 'num_comments', 'desc')\n",
    "march_df = fill_in_removed_text(march_df)\n",
    "database_to_sql(march_df, 'submissions', connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "april_df = scrape_monthly_submissions('medicine', 4, 'num_comments', 'desc')\n",
    "april_df = fill_in_removed_text(april_df)\n",
    "database_to_sql(april_df, 'submissions', connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "may_df = scrape_monthly_submissions('medicine', 5, 'num_comments', 'desc')\n",
    "may_df = fill_in_removed_text(may_df)\n",
    "database_to_sql(may_df, 'submissions', connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_df = scrape_monthly_submissions('medicine', 6, 'num_comments', 'desc')\n",
    "june_df = fill_in_removed_text(june_df)\n",
    "database_to_sql(june_df, 'submissions', connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "july_df = scrape_monthly_submissions('medicine', 7, 'num_comments', 'desc')\n",
    "july_df = fill_in_removed_text(july_df)\n",
    "database_to_sql(july_df, 'submissions', connection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
