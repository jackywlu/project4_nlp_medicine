{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 4: Natural Language Processing and Unsupervised Learning\n",
    "\n",
    "In this notebook, I test out how to preprocess a reddit comment.\n",
    "\n",
    "8/12 Notes:\n",
    "Workflow to be: get monthly post submissions -> add a column for cleaned_text, add a column for just nouns, just verbs, just adjectives, just named entities (?); do topic modeling on the nouns and named entities; do sentiment analysis on the cleaned_text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Imports](#section1)\n",
    "2. [Getting Post Data](#section2)\n",
    "3. [Exploratory Preprocessing](#section3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import praw\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "\n",
    "import psycopg2 as pg\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "### 2. Getting Post Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postgres info to connect\n",
    "\n",
    "connection_args = {\n",
    "    'host': 'localhost', # We are connecting to our local version of psql\n",
    "    'dbname': 'reddit_medicine',        # DB that we are connecting to\n",
    "    'port': 5432,        # port we opened on AWS\n",
    "    'password':'',\n",
    "    'user': 'postgres'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_postgres(connection_args):\n",
    "    '''\n",
    "    Connect to PostgreSQL database server\n",
    "    '''\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = pg.connect(**connection_args)\n",
    "    except (Exception, pg.DatabaseError) as error:\n",
    "        print(error)\n",
    "        \n",
    "    return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = connect_to_postgres(connection_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_submission_query = \"SELECT * FROM submissions WHERE month = 1;\"\n",
    "\n",
    "jan_df = pd.read_sql(jan_submission_query, connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jan_df.shape)\n",
    "jan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_df[\"full_text\"] = jan_df[\"title\"] + ' ' + jan_df[\"submission_text\"]\n",
    "jan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_corpus = ' '.join(jan_df[\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes about the information. There is a lot of new lines and some interesting &#x200B characters. There's also some markdown formatting. I want to keep the text in the square brackets, but eliminate the text with parentheses that start with https.\n",
    "\n",
    "There are some abbreviations (TL:DR; 75 yo CT (probably cat scan), MD, DO, NP, PA, ED that I might want to keep)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up \"\\n\" characters in corpus\n",
    "jan_corpus_1 = re.sub('\\n', ' ', jan_corpus)\n",
    "jan_corpus_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all links\n",
    "jan_corpus_2 = re.sub(r'https?:\\/\\/\\S+', ' ', jan_corpus)\n",
    "jan_corpus_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all weird &gt or &#x200B characters\n",
    "jan_corpus_3 = re.sub(r'&\\S*', ' ', jan_corpus)\n",
    "jan_corpus_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_jan_corpus = re.sub('\\n', ' ', jan_corpus)\n",
    "clean_jan_corpus = re.sub(r'https?:\\/\\/\\S+', ' ', clean_jan_corpus)\n",
    "clean_jan_corpus = re.sub(r'&\\S*', ' ', clean_jan_corpus)\n",
    "clean_jan_corpus = re.sub(r'\\\\xa0', ' ', clean_jan_corpus)\n",
    "characters_to_clean = '*>|[]()\\\\\",°'\n",
    "for c in characters_to_clean:\n",
    "    clean_jan_corpus = clean_jan_corpus.replace(c, '')\n",
    "clean_jan_corpus = clean_jan_corpus.replace('/', ' ') \n",
    "clean_jan_corpus = clean_jan_corpus.replace('-', ' ')\n",
    "clean_jan_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding common medical abbreviations\n",
    "abbreviations = re.findall('[A-Z][A-Z]+', clean_jan_corpus)\n",
    "set(abbreviations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common abbreviations I can find (manual):\n",
    "ICU, MD, DO, NP, PA, ED, DDS, MBA, EMS\n",
    "WHO, SARS, MERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviation_dict = {\"MD\": \"doctor_of_medicine\",\n",
    "                     \"MPH\": \"master_of_public_health\",\n",
    "                     \"MBA\": \"master_of_business_administrationi\",\n",
    "                     \"NP\": \"nurse_practitioner\",\n",
    "                     \"PA\": \"physician_assistant\",\n",
    "                     \"RN\": \"registered_nurse\",\n",
    "                     \"DVM\": \"doctor_of_veterinary_medicine\",\n",
    "                     \"DDS\": \"doctor_of_dentistry\",\n",
    "                     \"DO\": \"doctor_of_osteopathy\",\n",
    "                     \"ICU\": \"intensive_care_unit\",\n",
    "                     \"ER\": \"emergency_room\",\n",
    "                     \"EMS\": \"emergency_medical_services\",\n",
    "                     \"CDC\": \"centers_for_disease_control\",\n",
    "                     \"WHO\": \"world_health_organization\",\n",
    "                     \"SARS\": \"severe_acute_respiratory_syndrome\",\n",
    "                     \"MERS\": \"middle_east_respiratory_syndrome\",\n",
    "                     \"SOM\": \"school_of_medicine\"\n",
    "                     }\n",
    "abbreviation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in abbreviation_dict.keys():\n",
    "    clean_jan_corpus = clean_jan_corpus.replace(key, ' ' + abbreviation_dict[key] + ' ')\n",
    "clean_jan_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_jan_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_function(corpus, regex_patterns, char_space, char_no_space, abbrev_dict):\n",
    "    '''\n",
    "    Inputs:\n",
    "    - corpus (string): string of reddit posts\n",
    "    - regex_patterns (list): list of regex patterns to remove\n",
    "    - char_space (string): characters to replace with a space\n",
    "    - char_no_space (string): characters to replace with no space\n",
    "    - abbrev_dict (dict): dictionary to replace abbreviations with full words\n",
    "    Outputs:\n",
    "    - cleaned_corpus (string): cleaned string of reddit posts\n",
    "    '''\n",
    "    cleaned_corpus = str(corpus)\n",
    "    for pattern in regex_patterns:\n",
    "        cleaned_corpus = re.sub(pattern, ' ', cleaned_corpus)\n",
    "    \n",
    "    for char in char_space:\n",
    "        cleaned_corpus = cleaned_corpus.replace(char, ' ')\n",
    "    \n",
    "    for char in char_no_space:\n",
    "        cleaned_corpus = cleaned_corpus.replace(char, '')\n",
    "        \n",
    "    for key in abbrev_dict.keys():\n",
    "        cleaned_corpus = cleaned_corpus.replace(key, ' ' + abbrev_dict[key] + ' ')\n",
    "    \n",
    "    cleaned_corpus = cleaned_corpus.lower()\n",
    "    \n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_patterns = ['\\n', '\\t', r'https?:\\/\\/\\S+', r'&\\S*', r'\\\\xa0', r'[__]{2,}', r'[\\d]+', 'χ', '®']\n",
    "char_space = '^*>|[]()\",°#'\n",
    "char_no_space = '/-\\\\'\n",
    "abbrev_dict = {\"MD\": \"doctor_of_medicine\",\n",
    "               \"MPH\": \"master_of_public_health\",\n",
    "               \"MBA\": \"master_of_business_administrationi\",\n",
    "               \"NP\": \"nurse_practitioner\",\n",
    "               \"PA\": \"physician_assistant\",\n",
    "               \"RN\": \"registered_nurse\",\n",
    "               \"DVM\": \"doctor_of_veterinary_medicine\",\n",
    "               \"DDS\": \"doctor_of_dentistry\",\n",
    "               \"DO\": \"doctor_of_osteopathy\",\n",
    "               \"ICU\": \"intensive_care_unit\",\n",
    "               \"ER\": \"emergency_room\",\n",
    "               \"ED\": \"emergency_department\",\n",
    "               \"EMS\": \"emergency_medical_services\",\n",
    "               \"EMR\": \"electronic_medical_records\",\n",
    "               \"CFR\": \"case_fatality_rate\",\n",
    "               \"CT\": \"computed_tomography\",\n",
    "               \"CDC\": \"centers_for_disease_control\",\n",
    "               \"WHO\": \"world_health_organization\",\n",
    "               \"FDA\": \"food_and_drug_administration\",\n",
    "               \"SARS\": \"severe_acute_respiratory_syndrome\",\n",
    "               \"MERS\": \"middle_east_respiratory_syndrome\",\n",
    "               \"ARDS\": \"acute_respiratory_distress_syndrome\",\n",
    "               \"SOM\": \"school_of_medicine\",\n",
    "               \"COVID\": \"covid\",\n",
    "               \"N95\": \"n95\",\n",
    "               \"n95\": \"n95\", # make sure n95 is counted as a distinc\n",
    "               \"PPE\": \"personal_protective_equipment\"\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_jan_corpus = cleaning_function(jan_corpus, regex_patterns, char_space, char_no_space, abbrev_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_jan_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude megathreads that have repetitive post titles and post submission text\n",
    "full_submission_query = \"SELECT * FROM submissions WHERE title NOT LIKE 'Megathread:%' AND title NOT LIKE 'Megathread #%' and TITLE NOT LIKE 'Weekly Careers Thread';\"\n",
    "full_df = pd.read_sql(full_submission_query, connection)\n",
    "full_df[\"full_text\"] = full_df[\"title\"] + ' ' + full_df[\"submission_text\"]\n",
    "full_corpus = ' '.join(full_df[\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"cleaned_text\"] = full_df[\"full_text\"].apply(lambda x:\n",
    "                          cleaning_function(x, regex_patterns, char_space, char_no_space, abbrev_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_full_corpus = cleaning_function(full_corpus, regex_patterns, char_space, char_no_space, abbrev_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# might need to exclude megathreads because they repeat the same text over and over\n",
    "cleaned_full_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "### 3. Exploratory Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying out nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks the most promising?\n",
    "tokenize_by_word = word_tokenize(cleaned_full_corpus)\n",
    "tokenize_by_word[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not looking too great\n",
    "tokenize_by_sent = sent_tokenize(cleaned_full_corpus)\n",
    "tokenize_by_sent[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_by_bigram = word_tokenize(cleaned_full_corpus)\n",
    "twograms = list(ngrams(tokenize_by_bigram, 2))\n",
    "twograms[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegexpTokenizer with whitespace delimiter\n",
    "whitespace_tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n",
    "tokenize_by_regex = whitespace_tokenizer.tokenize(cleaned_full_corpus)\n",
    "tokenize_by_regex[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all text lowercase\n",
    "cleaned_full_corpus = cleaned_full_corpus.lower()\n",
    "cleaned_full_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "X = cv.fit_transform(word_tokenize(cleaned_full_corpus))\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "feature_names[-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more cleaning I need to do; lots of numbers\n",
    "\n",
    "#matches 2 or more underscores\n",
    "\n",
    "corpus_copy = re.sub('[__]{2,}',' ', cleaned_full_corpus)\n",
    "corpus_copy = re.sub('[\\d]+', ' ', corpus_copy)\n",
    "corpus_copy = re.sub('χ', ' ', corpus_copy)\n",
    "corpus_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(stop_words='english')\n",
    "X_1 = cv1.fit_transform(word_tokenize(corpus_copy))\n",
    "feature_names = cv1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing out stemmers now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_words = word_tokenize(corpus_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "lancaster_list = []\n",
    "for word in token_words:\n",
    "    lancaster_list.append(lancaster.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster_list[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "porter_list = []\n",
    "for word in token_words:\n",
    "    porter_list.append(porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_list[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer('english')\n",
    "snowball_list = []\n",
    "for word in token_words:\n",
    "    snowball_list.append(snowball.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_list[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now trying part of speech tag\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_of_speech = pos_tag(token_words)\n",
    "part_of_speech[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_nouns = []\n",
    "for word, pos in part_of_speech:\n",
    "    if 'NN' in pos:\n",
    "        only_nouns.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_nouns[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(only_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.help import upenn_tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems like spacy does a lot of things already in the pipeline\n",
    "# it has a part of speech tagger, a dependency parser, a named\n",
    "# entity recognizer, and a text classifier\n",
    "\n",
    "# if I want to use the named entity recognizer, I shouldn't spell out abbreviations?\n",
    "\n",
    "nlp = English()\n",
    "spacy_corpus = corpus_copy[:100000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_test = nlp(spacy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_post_extract(text, pos):\n",
    "    '''\n",
    "    Use spacy to extract words that are a specific part of speech from text.\n",
    "    Inputs:\n",
    "    - text (str): string of words to extract words from\n",
    "    - pos (list): list of part of speech strings; must be one of \"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"PROPN\"\n",
    "    https://spacy.io/api/annotation#pos-tagging this link contains a table with all the different parts of speech\n",
    "    Output:\n",
    "    - pos_string (str): string of words that are a specific part of speech\n",
    "    '''\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    spacy_text = nlp(text)\n",
    "    \n",
    "    pos_list = []\n",
    "    \n",
    "    for token in spacy_text:\n",
    "        for part_of_speech in pos:\n",
    "            if token.pos_ == part_of_speech:\n",
    "                pos_list.append(token.text)\n",
    "    \n",
    "    pos_string = ' '.join(pos_list)\n",
    "    return pos_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Our current enemy was still in the shadows. The wards slowly emptied with activities freed.\"\n",
    "pos = [\"NOUN\", \"VERB\"]\n",
    "spacy_post_extract(sample_text, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_2 = spacy.load(\"en_core_web_sm\")\n",
    "spacy_test_2 = nlp_2(spacy_corpus)\n",
    "for token in spacy_test_2[300:350]:\n",
    "    print(token.i, token.text, token.is_alpha, token.is_punct, token.like_num, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/api/annotation#pos-tagging list of all parts of speech; may need NOUN and PROPN (proper noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try gettting all nouns with spacy\n",
    "noun_list = []\n",
    "for token in spacy_test_2:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        noun_list.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_list[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting min_df to 2 helps clear away a lot of junk!\n",
    "count_vect = CountVectorizer(analyzer = 'word', stop_words = 'english', min_df=2)\n",
    "doc_word = count_vect.fit_transform(full_df[\"cleaned_text\"])\n",
    "words = count_vect.get_feature_names()\n",
    "vocab = count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec_df = pd.DataFrame(doc_word.toarray(), columns=count_vect.get_feature_names())\n",
    "count_vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying out tfidf vectorizer\n",
    "tfidf_vect = TfidfVectorizer(analyzer = 'word', stop_words = 'english', min_df=2)\n",
    "tfidf_doc = tfidf_vect.fit_transform(full_df[\"cleaned_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec_df = pd.DataFrame(tfidf_doc.toarray(), columns=tfidf_vect.get_feature_names())\n",
    "tfidf_vec_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Able to successfully do countvectorizer and tfidf vectorizer. However, a column of the column names look like junk. So it looks like before I do the vectorizer, I should stem beforehand.\n",
    "\n",
    "I also want to look at the posts per month rather than per the entire year. So my vectorizer should vectorize on a monthly basis instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying using cosine similarities to find similar posts\n",
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(combinations(enumerate(full_df[\"cleaned_text\"]), 2))\n",
    "combos = [(a[0], b[0]) for a, b in pairs]\n",
    "phrases = [(a[1], b[1]) for a, b in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [cosine_similarity(count_vec_df.iloc[[a]], count_vec_df.iloc[[b]]) for a, b in combos]\n",
    "doc_similarity_count = sorted(zip(results, phrases), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will need to eliminate weekly careers thread\n",
    "doc_similarity_count[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_similarity_count[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_named_entities (text):\n",
    "    '''\n",
    "    Use spacy to extract named entities from text\n",
    "    Inputs:\n",
    "    - text (str): string of words to extract words from\n",
    "    Output:\n",
    "    - ent_string (str): string of words that are named entities\n",
    "    '''\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    spacy_text = nlp(text)\n",
    "    \n",
    "    ent_list = []\n",
    "    \n",
    "    for ent in spacy_text.ents:\n",
    "        ent_list.append((ent.text, ent.label_))\n",
    "    return ent_list\n",
    "#     ent_string = ' '.join(ent_list)\n",
    "#     return ent_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = ' '.join(full_df[\"full_text\"].to_list())\n",
    "full_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at all spacy entities; could be interesting to look at all the geopolitical entities (is China mentioned more?)\n",
    "spacy_named_entities(' '.join(full_df[\"full_text\"].to_list())[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_vect = CountVectorizer(analyzer = 'word', stop_words = 'english', min_df=2)\n",
    "#doc_word = count_vect.fit_transform(full_df[\"cleaned_text\"])\n",
    "lsa = TruncatedSVD(3)\n",
    "doc_topic = lsa.fit_transform(doc_word)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = []\n",
    "for i in range(1, 4):\n",
    "    index_list.append(f'component_{i}')\n",
    "index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "                          index = index_list,\n",
    "                          columns=count_vect.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa, count_vect.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying out NMF\n",
    "nmf_model = NMF(3)\n",
    "doc_topic_1 = nmf_model.fit_transform(doc_word)\n",
    "topic_word_1 = pd.DataFrame(nmf_model.components_.round(3),\n",
    "                            index=index_list,\n",
    "                            columns=count_vect.get_feature_names())\n",
    "topic_word_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(nmf_model, count_vect.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to grab comments from one submission ignore the bottom\n",
    "url_template = (r'https://api.pushshift.io/reddit/search/comment/?link_id={}&limit=1000&sort_type={}&sort={}')\n",
    "submission_id = 'fjj0lr'\n",
    "sort = 'desc'\n",
    "sort_type = 'score'\n",
    "filled_in_template = url_template.format(submission_id, sort, sort_type)\n",
    "request = requests.get(filled_in_template)\n",
    "assert request.status_code == 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response = request.json()\n",
    "march_comment_list = []\n",
    "comment_dict = {}\n",
    "\n",
    "comment_dict['id'] = submission_id\n",
    "every_comment = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a dataframe of comments from one submission\n",
    "\n",
    "march_comment_list = []\n",
    "\n",
    "comment_dict = {}\n",
    "comment_dict['id'] = submission_id\n",
    "every_comment = ''\n",
    "\n",
    "for i in range(len(json_response['data'])):\n",
    "    comment = json_response['data'][i]['body']\n",
    "    \n",
    "    # basic cleanup\n",
    "    comment = comment.replace('\\n','').replace('^', '').replace(\"\\\\'\",\"'\")\n",
    "    \n",
    "    every_comment += comment + ' '\n",
    "\n",
    "comment_dict['all_comments'] = every_comment\n",
    "\n",
    "march_comment_list.append(comment_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "march_comment_df = pd.DataFrame(march_comment_list)\n",
    "march_comment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comment = march_comment_df.loc[0, \"all_comments\"]\n",
    "test_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the nlp spacy object\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_doc = nlp(test_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at first glance, it looks like the apostrophes aren't keeping the words together\n",
    "# the word \"that's\" has been separated into \"that\" and \"'s\"\n",
    "for token in spacy_doc[300:350]:\n",
    "    # token i is index; text is the test; alpha (alphanumeric), punctuation, or resembles a number\n",
    "    print(token.i, token.text, token.is_alpha, token.is_punct, token.like_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_2 = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_doc_2 = nlp_2(test_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COVID-19 is thought of as a number?; can get part of speech; syntactic dependencies\n",
    "for token in spacy_doc_2[300:350]:\n",
    "    print(token.i, token.text, token.is_alpha, token.is_punct, token.like_num, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at named entities\n",
    "for ent in spacy_doc_2[:1000].ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"FAC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"dobj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match patterns\n",
    "[{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example is nlp = spacy.load(\"en_core_web_sm\"); matcher = Matcher(nlp.vocab)\n",
    "# can use matches and operators to find text\n",
    "\n",
    "matcher = spacy.matcher.Matcher(nlp_2.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\": \"COVID-19\"}]\n",
    "matcher.add(\"COVID_PATTERN\", None, pattern)\n",
    "sample_doc = nlp_2(\"COVID-19 coronavirus pandemic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(sample_doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = sample_doc[start:end]\n",
    "    print(match_id, start, end, matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "### 4. Aug 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals for today include:\n",
    "* getting the top words for a section\n",
    "* stemming and lemmatizing and add those as columns\n",
    "* using bigrams or trigrams to do topic modeling\n",
    "* getting sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the top words for section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to future Jacky: use this function!\n",
    "def find_top_words_per_post(text: str, n: int):\n",
    "    '''\n",
    "    This function returns the top n words per reddit post.\n",
    "    Inputs:\n",
    "    - text (str): reddit submission post\n",
    "    - n (int): number of words\n",
    "    Outputs:\n",
    "    - list_of_top_words (list): a list of the top n words in the post\n",
    "    '''\n",
    "    tokenize_text = word_tokenize(text)\n",
    "    word_counter = Counter(tokenize_text)\n",
    "    list_of_top_words = [word for word, word_counter in word_counter.most_common(n)]\n",
    "    \n",
    "    if n < len(list_of_top_words):\n",
    "        return list_of_top_words[:n]\n",
    "    else:\n",
    "        return list_of_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words = full_df.loc[2, \"cleaned_text\"].strip()\n",
    "find_top_words_per_post(example_words, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_words_per_post(text: str, n: int):\n",
    "    '''\n",
    "    This function returns the top n words per reddit post.\n",
    "    Inputs:\n",
    "    - text (str): reddit submission post\n",
    "    - n (int): number of words\n",
    "    Outputs:\n",
    "    - list_of_top_words (list): a list of the top n words in the post\n",
    "    '''\n",
    "#     tokenize_text = word_tokenize(text)\n",
    "#     tokenize_df = pd.DataFrame(tokenize_text)\n",
    "#     list_of_top_words = tokenize_df[0].value_counts().index.to_list()\n",
    "#     if n < len(list_of_top_words):\n",
    "#         return list_of_top_words[:n]\n",
    "#     else:\n",
    "#         return list_of_top_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_text_1 = word_tokenize(example_df.loc[0,\"cleaned_text\"])\n",
    "tokenize_df_1 = pd.DataFrame(tokenize_text_1)\n",
    "list_of_top_words_1 = tokenize_df_1[0].value_counts().index.to_list()\n",
    "list_of_top_words_1[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My next goal is to try stemming and lemmatizing, then making bigrams or trigrams to use with topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df = full_df.copy().iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df[\"tokenize_text\"] = example_df[\"cleaned_text\"].apply(lambda x:word_tokenize(x))\n",
    "example_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    #snowball = SnowballStemmer('english')\n",
    "    stem_list = [snowball.stem(word) for word in text]\n",
    "    stem_string = ' '.join(stem_list)\n",
    "    return stem_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df[\"stemmed_text\"] = example_df[\"tokenize_text\"].apply(stem_text)\n",
    "example_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatizer(text):\n",
    "    spacy_nlp = English()\n",
    "    doc = spacy_nlp(text)\n",
    "    \n",
    "    lemmatize_list = []\n",
    "    for token in doc:\n",
    "        lemmatize_list.append(token.lemma_)\n",
    "    return ' '.join(lemmatize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df[\"lemmatized_text\"] = example_df[\"cleaned_text\"].apply(spacy_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_test_1 = CountVectorizer(analyzer = 'word', stop_words = 'english', min_df = 2, ngram_range=(2,2))\n",
    "doc_word_1 = count_vect_test_1.fit_transform(example_df[\"cleaned_text\"])\n",
    "lsa = TruncatedSVD(3)\n",
    "doc_topic_1 = lsa.fit_transform(doc_word_1)\n",
    "\n",
    "index_list = []\n",
    "\n",
    "for i in range(1, 4):\n",
    "    index_list.append(f'component_{i}')\n",
    "\n",
    "topic_word_1 = pd.DataFrame(lsa.components_.round(3),\n",
    "                            index=index_list,\n",
    "                            columns=count_vect_test_1.get_feature_names())\n",
    "display_topics(lsa, count_vect_test_1.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_test_2 = CountVectorizer(analyzer = 'word', stop_words = 'english', min_df = 2, ngram_range=(2,2))\n",
    "doc_word_2 = count_vect_test_2.fit_transform(example_df[\"stemmed_text\"])\n",
    "lsa = TruncatedSVD(3)\n",
    "doc_topic_2 = lsa.fit_transform(doc_word_2)\n",
    "\n",
    "index_list = []\n",
    "\n",
    "for i in range(1, 4):\n",
    "    index_list.append(f'component_{i}')\n",
    "\n",
    "topic_word_2 = pd.DataFrame(lsa.components_.round(3),\n",
    "                            index=index_list,\n",
    "                            columns=count_vect_test_2.get_feature_names())\n",
    "display_topics(lsa, count_vect_test_2.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_test_3 = CountVectorizer(analyzer = 'word', stop_words = 'english', min_df = 2, ngram_range=(2,2))\n",
    "doc_word_3 = count_vect_test_3.fit_transform(example_df[\"lemmatized_text\"])\n",
    "lsa = TruncatedSVD(3)\n",
    "doc_topic_3 = lsa.fit_transform(doc_word_3)\n",
    "\n",
    "index_list = []\n",
    "\n",
    "for i in range(1, 4):\n",
    "    index_list.append(f'component_{i}')\n",
    "\n",
    "topic_word_3 = pd.DataFrame(lsa.components_.round(3),\n",
    "                            index=index_list,\n",
    "                            columns=count_vect_test_3.get_feature_names())\n",
    "display_topics(lsa, count_vect_test_3.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_test_4 = CountVectorizer(analyzer = 'word', stop_words = 'english', min_df = 2, ngram_range=(2,2))\n",
    "doc_word_4 = count_vect_test_4.fit_transform(example_df[\"cleaned_text\"])\n",
    "nmf_model = NMF(3)\n",
    "doc_topic_4 = nmf_model.fit_transform(doc_word_4)\n",
    "\n",
    "index_list = []\n",
    "\n",
    "for i in range(1, 4):\n",
    "    index_list.append(f'component_{i}')\n",
    "\n",
    "topic_word_4 = pd.DataFrame(nmf_model.components_.round(3),\n",
    "                            index=index_list,\n",
    "                            columns=count_vect_test_4.get_feature_names())\n",
    "display_topics(nmf_model, count_vect_test_4.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_test_5 = CountVectorizer(analyzer = 'word', stop_words = 'english', min_df = 2, ngram_range=(2,2))\n",
    "doc_word_5 = count_vect_test_5.fit_transform(example_df[\"stemmed_text\"])\n",
    "nmf_model = NMF(3)\n",
    "doc_topic_5 = nmf_model.fit_transform(doc_word_5)\n",
    "\n",
    "index_list = []\n",
    "\n",
    "for i in range(1, 4):\n",
    "    index_list.append(f'component_{i}')\n",
    "\n",
    "topic_word_5 = pd.DataFrame(nmf_model.components_.round(3),\n",
    "                            index=index_list,\n",
    "                            columns=count_vect_test_5.get_feature_names())\n",
    "display_topics(nmf_model, count_vect_test_5.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_test_6 = CountVectorizer(analyzer = 'word', stop_words = 'english', min_df = 2, ngram_range=(2,2))\n",
    "doc_word_6 = count_vect_test_6.fit_transform(example_df[\"lemmatized_text\"])\n",
    "nmf_model = NMF(3)\n",
    "doc_topic_6 = lsa.fit_transform(doc_word_6)\n",
    "\n",
    "index_list = []\n",
    "\n",
    "for i in range(1, 4):\n",
    "    index_list.append(f'component_{i}')\n",
    "\n",
    "topic_word_6 = pd.DataFrame(lsa.components_.round(3),\n",
    "                            index=index_list,\n",
    "                            columns=count_vect_test_6.get_feature_names())\n",
    "display_topics(lsa, count_vect_test_6.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(vectorizer, text, model, n_words):\n",
    "    '''\n",
    "    This function prints out topics based on the model on the vectorized text.\n",
    "    \n",
    "    Inputs:\n",
    "    - vectorizer: word vectorized used to vectorize the text\n",
    "    - text: text to be analyzed\n",
    "    - model (topic modeling model): NMF, LDA, other topic modeling models\n",
    "    - n_words (int): \n",
    "    \n",
    "    Outputs:\n",
    "    - prints out topics with n (corresponding to n_words) words that relate to that topic\n",
    "    '''\n",
    "    vectorized_text = vectorizer.fit_transform(text)\n",
    "    transform_text = model.fit_transform(vectorized_text)\n",
    "    display_topics(model, vectorizer.get_feature_names(), n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topic_dataframe(vectorizer, text, model, n_topics):\n",
    "        '''\n",
    "    This function prints out n (corresponding to n_topics) number of topics based on\n",
    "    the model on the vectorized text.\n",
    "    \n",
    "    Inputs:\n",
    "    - vectorizer: word vectorized used to vectorize the text\n",
    "    - text: text to be analyzed\n",
    "    - model (topic modeling model): NMF, LDA, other topic modeling models\n",
    "    - n_topics (int): number of topics\n",
    "    \n",
    "    Outputs:\n",
    "    - topic_df (DataFrame): \n",
    "    '''\n",
    "    \n",
    "    vectorized_text = vectorizer.fit_transform(text)\n",
    "    \n",
    "    transform_text = model.fit_transform(vectorized_text)\n",
    "    \n",
    "    index_list = []\n",
    "    \n",
    "    for i in range(1, len(n_topics) + 1):\n",
    "        index_list.append(f'component_{i}')\n",
    "        \n",
    "    topic_df = pd.DataFrame(model.components_.round(3),\n",
    "                            index=index_list,\n",
    "                            columns=vectorizer.get_feature_names()\n",
    "                           )\n",
    "    \n",
    "    return topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My last goal for Aug 13 is to do sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_string = example_df.loc[0,\"full_text\"]\n",
    "example_string[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.polarity_scores(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.polarity_scores(example_string)['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compound score is between -1 and 1. It is described as the \"normalized, weighted composite socre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df[\"compound_sentiment\"] = example_df[\"full_text\"].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "example_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df[\"textblob_polarity\"] = example_df[\"full_text\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "example_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding stop words\n",
    "\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "add_stop_words = ['example']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do I need for LDA?\n",
    "* Corpus, num_topics, random_state, chunksize, passes, alpha\n",
    "* dictionary (corpora.Dictionary(data_lemmatized)\n",
    "* for corpus, you have to make a term document frequency\n",
    "* goal is to build many LDA models with different values of number of topics and get the one that gives the highest coherence value\n",
    "Example: \n",
    "```python\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aug 15 Work\n",
    "* Trying to make a class for NLP pipeline\n",
    "* Trying out LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlp_preprocessor:\n",
    "    '''\n",
    "    A class for pipelining our NLP data. The user provides the text,\n",
    "    and this class manages the cleaning, transforming, and other\n",
    "    modifications of the text data.\n",
    "    \n",
    "    Parameters:\n",
    "    vectorizer: model to vectorize text data\n",
    "    tokenizer: tokenizer to use; defaults to splitting on spaces\n",
    "    cleaning_function: how to clean the data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 vectorizer=CountVectorizer(),\n",
    "                 tokenizer=None,\n",
    "                 cleaning_function=None,\n",
    "                 stemmer=None,\n",
    "                 model=None):\n",
    "        \n",
    "        if not tokenizer:\n",
    "            tokenizer = self.splitter\n",
    "        if not cleaning_function:\n",
    "            cleaning_function = self.clean_text\n",
    "        self.stemmer = stemmer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.cleaning_function = cleaning_function\n",
    "        self.vectorizer = vectorizer\n",
    "        self._is_fit = False\n",
    "        \n",
    "    def splitter(self, text):\n",
    "        '''\n",
    "        Default tokenizer that splits on spaces\n",
    "        '''\n",
    "        return text.split(' ')\n",
    "    \n",
    "    def clean_text(self, text, tokenizer, stemmer):\n",
    "        '''\n",
    "        Naive function to lowercase all words and clean them\n",
    "        quickly. This is the default if no other cleaning\n",
    "        function is specified\n",
    "        '''\n",
    "        cleaned_text = []\n",
    "        \n",
    "        for post in text:\n",
    "            cleaned_words = []\n",
    "            for word in tokenizer(post):\n",
    "                lower_word = word.lower()\n",
    "                if stemmer:\n",
    "                    lower_word = stemmer.stem(lower_word)\n",
    "                cleaned_words.append(lower_word)\n",
    "            cleaned_text.append(' '.join(cleaned_words))\n",
    "        return cleaned_text\n",
    "    \n",
    "    def fit(self, text):\n",
    "        '''\n",
    "        Cleans the data and then fits the vectorizer to the text\n",
    "        '''\n",
    "        clean_text = self.cleaning_function(text, self.tokenizer, self.stemmer)\n",
    "        self.vectorizer.fit(clean_text)\n",
    "        self._if_fit = True\n",
    "    \n",
    "    def transform(self, text):\n",
    "        '''\n",
    "        Cleans the text and transforms it into a vectorized format.\n",
    "        Returns the vectorized form of the data.\n",
    "        '''\n",
    "        \n",
    "        if not self._is_fit:\n",
    "            raise ValueError(\"Must fit model before transforming!\")\n",
    "        \n",
    "        clean_text = self.cleaning_function(text, self.tokenizer, self.stemmer)\n",
    "        return self.vectorizer.transform(clean_text)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from pprint import pprint\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_corpus = cleaned_full_corpus[:100000]\n",
    "example_tokenize = word_tokenize(example_corpus)\n",
    "example_tokenize = [[word for word in example_tokenize if word not in stop_words]]\n",
    "example_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_dictionary = corpora.Dictionary(example_tokenize)\n",
    "tokenized_text = example_tokenize\n",
    "\n",
    "corpus = [lda_dictionary.doc2bow(text) for text in tokenized_text]\n",
    "\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=lda_dictionary,\n",
    "                                            num_topics=10,\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=50,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, lda_dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to take for cleaning\n",
    "* Tokenize, and then remove punctuation\n",
    "\n",
    "example: <br/>\n",
    "doc = nlp(text_with_punct)<br/>\n",
    "tokens = [t.text for t in doc] # python based removal<br/>\n",
    "tokens_without_punct_python = [t for t in tokens if t not in string.punctuation]<br/>\n",
    "or # spacy based removal<br/>\n",
    "tokens_without_punct_spacy = [t.text for t in doc if t.pos_ != 'PUNCT']<br/>\n",
    "\n",
    "* Normalise data (change numbers to text and abbreviations too?); based on nltk\n",
    "```python\n",
    "from normalise import normalise\n",
    "\n",
    "user_abbr = {\n",
    "    \"N.A.T.O\": \"North Atlantic Treaty Organization\"\n",
    "}\n",
    "\n",
    "normalized_tokens = normalise(word_tokenize(text), user_abbrevs=user_abbr, verbose=False)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
